## Архив HTML-кода вики（Py-crawler）

Это используется для создания проекта сканера, который сканирует все страницы, присутствующие в вики, на наличие статических версий веб-сайта только для чтения после аварии. Он может не подходить для восстановления проекта, но служит основой для страницы восстановления.
Из-за технических проблем в настоящее время невозможно писать сканеры на C++, плюс доступ к wikidot медленный, поэтому он немного медленный, но 60 раз / мин примерно то же самое.
В настоящее время это относится ко всем сайтам Wikidot, но для предотвращения вредоносных сканеров любой сайт должен иметь страницу для сканирования.

### **Требования к конфигурации**
```
У вас должен быть Python 3 на вашем компьютере (последняя версия может работать лучше)
Создайте страницу на вашем сайте URL / страницы и добавьте [[module Pages preview="true"]] код
```

### **Установка зависимостей**
Заметка! При установке python3 вам нужно выбрать «Добавить Python в PATH», чтобы использовать команду pip.
Установите файлы библиотеки в cmd или PowerShell со следующим кодом:
```
pip install requests
pip install bs4
```

### **Установка бота**
Пожалуйста, нажмите `Releases` боковую панель чтобы отобразить последнюю версию, выберите установочный файл в конце .exe установщика. Он установит файлы в тот же каталог. Все, что вам нужно сделать, это нажать соответствующую кнопку на клавиатуре, чтобы активировать соответствующую команду.

Пользователи Linux компилируют свои собственные. Сгенерированные a.html могут быть удалены

### **Гусеничный робот запускается**
* Включите RunMe.vbs (GUI) или main.exe (CLI) во время выполнения.
* Введите URL-адрес в графическое поле ввода (GUI) или область ввода командной строки (CLI) и нажмите «Подтвердить».
* Посмотрите на номер после страницы X из X, отображаемый под URL/страницей вашего сайта (в данном случае следующий x)
* Введите это число в поле ввода командной строки {если вы заполните слишком много (пусть это число будет X, X≠0), он выведет HTML-файл 1 ~ X (имя pages1 ~ pagesX.html), но (X-1) и X одинаковы}, а затем обернет.
* Дождитесь завершения программы. (Если вы обнаружите, что страница имеет несколько сканирований, это нормально, программа будет сканировать все ссылки, которые на ней существуют)
*Договорились? Не забудьте упаковать в другую папку, чтобы избежать путаницы.

### **反馈地址**
ВОЗНИКЛА ОШИБКА ИЛИ БАГ? Приходите к обратной связи [здесь](http://ld-private-website.wikidot.com/forum/c-7602918/pyc) или приходите [сюда](https://github.com/TimeLine-Bookstore/Py-crawler/issues) обратной связи.

У вас есть идея для новой функции, но вы не знаете, где дать обратную связь? Приходите к обратной связи [здесь](http://ld-private-website.wikidot.com/forum/t-15402049/pyc-1-1-0-1-9) или приходите [сюда](https://github.com/TimeLine-Bookstore/Py-crawler/issues) обратной связи.

Можете ли вы оказать нам техническую помощь? Приходите к обратной связи [здесь](http://ld-private-website.wikidot.com/forum/c-7602920/) или [здесь](https://github.com/TimeLine-Bookstore/Py-crawler/fork) вытягивайте ветви, чтобы внести изменения.

----------
```
Copyright (c) 2022 TimeLine-Bookstore
All Rights Reserved.
Авторское право (c) 2022 Хронология Книжный магазин
Все права защищены.
```
